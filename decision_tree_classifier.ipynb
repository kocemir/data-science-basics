{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw1-koc-emirhan",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voHKcAfRfdNY"
      },
      "source": [
        "\n",
        "\n",
        "## Goal\n",
        "\n",
        "The goal of this study is three-fold:\n",
        "\n",
        "*   Introduction to the machine learning experimental set up \n",
        "*   Gain experience with Decision tree approache\n",
        "*   Gain experience with the Scikit library\n",
        "\n",
        "## Dataset\n",
        "**MNIST** is a collection of 28x28 grayscale images of digits (0-9); hence each pixel is a gray-level from 0-255. \n",
        "\n",
        "**Download the data from Keras. You must use a 20% of the training data for validation** (no need for cross-validation as you have plenty of data) and **use the official test data (10,000 samples) only for testing.**\n",
        "\n",
        "## Task \n",
        "Build a decision tree classifier with the scikit library function calls to classify digits in the MNIST dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YOYiWvHbNDW"
      },
      "source": [
        "##1) Initialize\n",
        "\n",
        "*   First make a copy of the notebook given to you as a starter.\n",
        "\n",
        "*   Make sure you choose Connect form upper right.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM-wwHR8qL0M"
      },
      "source": [
        "## 2) Load training dataset\n",
        "\n",
        "*  Read from Keras library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz3iMpjVfa5I",
        "outputId": "8590d366-dc0e-4f8b-a8bf-a9cab62ba851",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load the Pandas libraries with alias 'pd' \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from keras import datasets\n",
        "\n",
        "\n",
        "# Read data \n",
        "\n",
        "(X_train,y_train), (X_test,y_test)= datasets.mnist.load_data()   # I read data from keres dataset as two differet tupple of train and test\n",
        "                                                                #  with their corresponding features and labels\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzQ0XWLkzYll"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NdW2ItjHLxJ"
      },
      "source": [
        "##3) Understanding the dataset\n",
        "\n",
        "There are alot of functions that can be used to know more about this dataset\n",
        "\n",
        "- What is the shape of the training set (num of samples X number of attributes) ***[shape function can be used]***\n",
        "\n",
        "- Display attribute names ***[columns function can be used]***\n",
        "\n",
        "- Display the first 5 rows from training dataset ***[head or sample functions can be used]***\n",
        "\n",
        "Note: Understanding the features, possibly removing some features etc. is an important part in building an ML system, but for this homework this is not really necessary as  the features are homogeneous (pixels) and all necessary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA_AjGQasjvS",
        "outputId": "23936bde-20e5-40da-d6cd-1e810a95f987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# print shape\n",
        "\n",
        "\n",
        "# I checked how many pixels in a any of the photo exist, it was given the form of (28x28)\n",
        "image_height=X_train.shape[1]                 \n",
        "image_width =X_train.shape[2]\n",
        "dim_image= image_height*image_width   # I found the cardinality of the each photo\n",
        "dim_data= (X_train.shape[0],dim_image)  # I loaded the dimension to a tupple\n",
        "print('Data Dimensionality: ', dim_data)  \n",
        "\n",
        "\n",
        "# Converted 2D image to 1D array \n",
        "train_X = (np.reshape(X_train,dim_data))   # At this point, I reshaped the data as 60000,784\n",
        "test_X =(np.reshape(X_test, (X_test.shape[0], dim_image))) # Reshaped test data as 10000,784\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Conversion to pandas dataframes to see data head and attributes\n",
        "\n",
        "train_label= pd.DataFrame(y_train) \n",
        "test_label = pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "data_train= pd.DataFrame(train_X)\n",
        "data_test = pd.DataFrame(test_X)\n",
        "\n",
        "data_train['labels']=train_label  # I added labels as a column at the end of the data\n",
        "data_test['labels']=test_label\n",
        "\n",
        "# print first 5 rows in your dataset\n",
        "print('Head of Data:')\n",
        "data_train.head(5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Dimensionality:  (60000, 784)\n",
            "Head of Data:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>252</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1  2  3  4  5  6  7  8  ...  776  777  778  779  780  781  782  783  labels\n",
              "0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0       5\n",
              "1  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0       0\n",
              "2  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0       4\n",
              "3  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0       1\n",
              "4  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0       9\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vop4rwZVxh9Z"
      },
      "source": [
        "##4) Shuffle and Split TRAINING data as train (also called development) (80%) and validation (20%) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEhk8R24xhdY"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Shuffle the training data\n",
        "\n",
        "#Shuffled the train data\n",
        "train_x, train_y = shuffle(train_X, y_train)\n",
        "\n",
        "\n",
        "# Split 80-20\n",
        "\n",
        "#Splitted data as development and validation\n",
        "(dev_x, valid_x, dev_y, valid_y)= train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR1oMsPu0AV_"
      },
      "source": [
        "##5) Train a decision tree classifier on development/train data and do model selection using the validation data\n",
        "\n",
        "* Train 3 decision tree classifiers with different values of \"min_samples_split\" which is the minimum number of samples required to split an internal node:  min_samples_split = [default = 2, 5, 10]. \n",
        "* Test the 3 models on validation set and choose the best one.\n",
        "* Plot the train and validation set errors for those 3 settings - on one plot. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv6oac-T3Wy5"
      },
      "source": [
        "\n",
        "# Train decision tree classifiers\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "vals = [2,5,10] \n",
        "acc_dev = np.array([])\n",
        "acc_valid =np.array([])\n",
        "error_valid = []\n",
        "error_dev = []\n",
        " \n",
        "\n",
        "for ind in vals:\n",
        " clf = tree.DecisionTreeClassifier(min_samples_split=ind,random_state=0)\n",
        " clf = clf.fit(dev_x,dev_y)                     # Fitted development data and learned the model\n",
        " \n",
        " predicted_valid = clf.predict(valid_x)         # According to model, predicted labels in validation set\n",
        " acc_valid=np.append(acc_valid, accuracy_score(valid_y,predicted_valid)) \n",
        " error_valid.append(1-accuracy_score(valid_y, predicted_valid)) # For each sample split, found an error, put it on a list\n",
        " \n",
        " predicted_dev =clf.predict(dev_x)              # Also, wanted to see how my model is fine in train set to see if there is overfitting or not\n",
        " acc_dev= np.append(acc_dev, accuracy_score(dev_y,predicted_dev))\n",
        " error_dev.append(1-accuracy_score(dev_y, predicted_dev))\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hQbDTUOI_aC",
        "outputId": "e13b2cc0-c913-4db8-ed2e-fecbef4bba8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Respectively accuracy in validation set for 2,5,10 is ', acc_valid)\n",
        "print('Respectively accuracy in development set for 2,5,10 is ', acc_dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Respectively accuracy in validation set for 2,5,10 is  [0.86725    0.86666667 0.8635    ]\n",
            "Respectively accuracy in development set for 2,5,10 is  [1.         0.98235417 0.9643125 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-bEjRq_CnZF",
        "outputId": "cb30ca07-7488-4f5c-b3f4-a6b8b0458207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "\n",
        "# Plot errors\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(vals,error_valid)\n",
        "plt.scatter(vals,error_dev)\n",
        "plt.legend( ['Val_Error', 'Dev_Error'])\n",
        "plt.title('Error vs min_samples_split')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boqe46St1--f"
      },
      "source": [
        "## 7) Test your CHOSEN classifier on Test set\n",
        "\n",
        "- Load test data\n",
        "- Apply same pre-processing as training data (probably none)\n",
        "- Predict the labels of testing data **using the best chosen SINGLE model out of the models that you have tried from step 6 (you have selected your model according to your validation results)** and report the accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPLke8jyFGng"
      },
      "source": [
        "\n",
        "# Load test data\n",
        "\n",
        "(X_train,y_train), (X_test,y_test)= datasets.mnist.load_data()\n",
        "\n",
        "# Organize train and test datas\n",
        "\n",
        "X_train= np.reshape(X_train, (X_train.shape[0],X_train.shape[1]*X_train.shape[2]))  # Loaded all training data -> dev+valid\n",
        "X_test= np.reshape(X_test, (X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\n",
        "# Choose the best min_samples_split value with the best accuracy score\n",
        "\n",
        "ind= np.argmax(acc_valid)                                                           # Chosse the index of best accuracy to find the best tuning  min_samples_split value\n",
        "ind= vals[ind]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfTqxHI_7L99",
        "outputId": "2ff5d562-e31e-47d0-aff0-f9aa09624c08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "predicted_test = clf.predict(X_test) # Now, made prediction on test data\n",
        "acc= accuracy_score(predicted_test,y_test)*100  # Finaly obtained test accuracy\n",
        "print('Accuracy of test data is,', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of test data is, 87.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggm1FCAWQYce",
        "outputId": "6ec29983-b415-42d0-bc68-e0c98a270fdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, predicted_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 918,    2,    9,    7,    5,    9,   17,    8,    3,    2],\n",
              "       [   1, 1094,   10,    7,    1,    5,    4,    6,    6,    1],\n",
              "       [  15,   10,  873,   42,   11,   12,   11,   30,   16,   12],\n",
              "       [   8,    8,   25,  855,    8,   44,    9,    9,   27,   17],\n",
              "       [   7,    6,    7,    6,  854,   10,   22,   11,   11,   48],\n",
              "       [  21,    6,    9,   54,   10,  728,   15,    9,   21,   19],\n",
              "       [  12,    2,   12,    5,   27,   27,  850,    4,   11,    8],\n",
              "       [   4,    6,   36,   11,   12,    4,    1,  922,    8,   24],\n",
              "       [  20,    8,   27,   37,   23,   31,   21,   11,  773,   23],\n",
              "       [  16,    5,   12,   21,   46,   17,    4,   21,   22,  845]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDWIcnp7F8JS"
      },
      "source": [
        "\n",
        "```\n",
        "REPORT\n",
        "\n",
        "MNIST dataset consists of handwritten digits with  their images and \n",
        "corresponding label. In this homework, the problem was to create\n",
        "a model using Desicion Tree Classifier to be able to classify any\n",
        "given handwritten digits properly.\n",
        "When it comes to talk about sizes of train( development+validation)\n",
        "and test size, we can see that train set has 60000 samples while test \n",
        "set has 10000 samples.At later stages, train set is divided such \n",
        "that development and validation with ratio of 0.2.Just before this step,\n",
        "we shuffled train data to avoid bias. \n",
        "Before training the data, I reshaped it such that 28x28 matrix of pixels\n",
        "were converted to 1x784. I did this on both train and test datas. Also, \n",
        "to see head and columns of data, I converted numpy data matrix to\n",
        "pandas dataframes.Also, added labels as a column.\n",
        "\n",
        "Results: Accuracy of validation set with model trained with \n",
        "devolopment set is subsequently [0.86875    0.86841667 0.87041667]\n",
        "for min_samples_split values 2,5 and 10. Remarkably, accuracies in \n",
        "devolopment data was [1.  0.98191667 0.9650625 ]. In my opinion, the al-\n",
        "gorithm in prone to overfitting.\n",
        "I obtained maximum accuracy with 2 as min_samples_split, \n",
        "therefore I set it as parameter in  prediction of test \n",
        "data. Thus, accuracy in test was 87.21.\n",
        "In my opinion, we see the curse of dimensionality regarding\n",
        " the speed of algorith. As there are many features beside \n",
        " many samples, algorithm works slower than my expectations.\n",
        " In confusion matrix, row corresponds to true label and colums \n",
        " are predicted label.It is seen that number in diogonal are high\n",
        " which means model mostly classify correctly.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    }
  ]
}