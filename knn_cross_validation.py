# -*- coding: utf-8 -*-
"""HW2-koc-emirhan

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qYbS61mjZacz2q9_5roqHyh4aaf1djpn

# Goal

The goal of this study is to get familiar feature handling and cross validation.


# Dataset

German Credit Risk dataset, prepared by Prof. Hoffman, classifies each person as having a good or bad credit risk. The dataset that we use consists of both numerical and categorical features.



# Task

Build a k-NN classifier with scikit-learn library to classify people as bad or good risks for the german credit dataset.

# 1) Initialize

First, make a copy of this notebook in your drive
"""

# Mount to your drive, in this way you can reach files that are in your drive
# Run this cell
# Go through the link that will be showed below
# Select your google drive account and copy authorization code and paste here in output and press enter
# You can also follow the steps from that link
# https://medium.com/ml-book/simplest-way-to-open-files-from-google-drive-in-google-colab-fae14810674 

from google.colab import drive
drive.mount('/content/drive')

"""# 2) Load Dataset

To start working for your homework, take a copy of the folder, given in the below link to your own google drive. You find the train and test data under this folder.

[https://drive.google.com/drive/folders/1DbW6VxLKZv2oqFn9SwxAnVadmn1_nPXi?usp=sharing](https://drive.google.com/drive/folders/1DbW6VxLKZv2oqFn9SwxAnVadmn1_nPXi?usp=sharing)

After copy the folder, copy the path of the train and test dataset to paste them in the below cell to load your data.

"""

import pandas as pd
import numpy as np


# Imported test and train data

train_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/HW2/ML2021_HW2/german_credit_train.csv')
test_df = pd.read_csv("/content/drive/My Drive/Colab Notebooks/HW2/ML2021_HW2/german_credit_test.csv")

"""# 3) Optional - Analyze the Dataset 

You can use the functions of the pandas library to analyze your train dataset in detail - **this part is OPTIONAL - look around the data as you wish**.


*   Display the number of instances and features in the train ***(shape function can be used)**
*   Display 5 random examples from the train ***(sample function can be used)**
*   Display the information about each features ***(info method can be used)**


"""

# Print shape
print("Train data dimensionality: ", train_df.shape )

# Print random 5 rows
print("Examples from train data:", train_df.head())

# Print the information about the dataset
print("Information about train data :",train_df.info() )

train_df.head()

"""# 4) Define your train and test labels

*  Define labels for both train and test data in new arrays 
*  And remove the label column from both train and test sets do tht it is not used as a feature! 


(**you can use pop method**)

"""

# Define labels
risk_mapping = {1:0,2:1};

train_df['Risk'] = train_df['Risk'].replace(risk_mapping)
test_df['Risk'] = test_df['Risk'].replace(risk_mapping)

train_label = train_df['Risk'];
test_label = test_df['Risk']

train_df= train_df.drop(['Risk'],axis=1)
test_df=test_df.drop(['Risk'],axis=1)

train_df.head()

train_label.head() # O is bad, 1 is good

test_df.head()

"""# 5) Handle missing values if any 

*   Print the columns that have **NaN** values (**isnull** method can be used)
*   You can impute missing values with mode of that feature or remove samples or attributes
*   To impute the test set, you should use the mode values that you obtain from **train** set, as **you should not be looking at your test data to gain any information or advantage.**


"""

# Print columns with NaN values
train_df.isnull().any()

train_df.isnull().sum()

test_df.isnull().sum()

# Impute missing values by replacing with mode value
m= train_df['Housing'].mode()
m= m[0]
print(m)
train_df.fillna(m, inplace=True)
train_df.isnull().any()

m= test_df['Housing'].mode()[0]
test_df.fillna(m,inplace=True)
test_df.isnull().sum()

print(train_df)

print(test_df)

"""# 6) Transform categorical / ordinal features

* Transform all categorical / ordinal features using the methods that you have learnt in lectures and recitation 4 for both train and test data
* You saw the dictionary use for mapping in recitation. (You can use **replace function** to assign new values to the categories of a column).

*  The class of the categorical attributes in the dataset are defined as follows:
  - Status of existing checking account
     - A11 :      ... <    0 DM
	- A12 : 0 <= ... <  200 DM
	- A13 :      ... >= 200 DM / salary assignments for at least 1 year
     - A14 : no checking account

 - Credit history
    - A30 : no credits taken/all credits paid back duly
    - A31 : all credits at this bank paid back duly
	- A32 : existing credits paid back duly till now
    - A33 : delay in paying off in the past
	- A34 : critical account/other credits existing (not at this bank)

  - Savings account
    - A61 :          ... <  100 DM
	- A62 :   100 <= ... <  500 DM
	- A63 :   500 <= ... < 1000 DM
	- A64 :          .. >= 1000 DM
    - A65 :   unknown/ no savings account

 - Employment Since
    - A71 : unemployed
    - A72 :       ... < 1 year
	- A73 : 1  <= ... < 4 years  
	- A74 : 4  <= ... < 7 years
	- A75 :       .. >= 7 years
 
 - Personal Status
    - A91 : male   : divorced/separated
	- A92 : female : divorced/separated/married
    - A93 : male   : single
	- A94 : male   : married/widowed
	- A95 : female : single

  - Property
     -  A121 : real estate
	- A122 : if not A121 : building society savings agreement/life insurance
    - A123 : if not A121/A122 : car or other, not in attribute 6
	- A124 : unknown / no property

 - OtherInstallPlans  
    - A141 : bank
	- A142 : stores
	- A143 : none

 - Housing
    -  A151 : rent
	 - A152 : own
	- A153 : for free
"""

train_df.drop(columns=['PersonalStatus'],inplace = True)
test_df.drop(columns=['PersonalStatus'],inplace = True)

# Transform the categorical / ordinal attributes
checkpoint_map = {'A14':0,'A11':1,'A12':2,'A13':3}
train_df.replace(checkpoint_map, inplace=True)
test_df.replace(checkpoint_map,inplace = True)

credit_hist_map = {'A34':-2,'A33':-1,'A30':0.5,'A32':1,'A31':2}
train_df.replace(credit_hist_map, inplace=True)
test_df.replace(credit_hist_map,inplace = True)



save_acc_map = {'A61':1,'A65':0,'A62':2,'A63':3,'A64':4}
train_df.replace(save_acc_map, inplace=True)
test_df.replace(save_acc_map,inplace = True)


employment_map = {'A71':-4,'A72':1,'A73':2,'A74':3,'A75':4}
train_df.replace(employment_map, inplace=True)
test_df.replace(employment_map,inplace = True)

property_map = {'A121':3,'A122':2,'A123':1,'A124':-2}
train_df.replace(property_map, inplace=True)
test_df.replace(property_map,inplace = True)

housing_map ={'A151':0,'A152':3,'A153':1}
train_df.replace(housing_map, inplace=True)
test_df.replace(housing_map,inplace = True)

plans_map = {'A141':'bank','A142':'stores','A143':'none'}
train_df.replace(plans_map, inplace=True)
test_df.replace(plans_map, inplace = True)

dummies_plans = pd.get_dummies(train_df['OtherInstallPlans'],prefix='plans')
dummies_plans.head()


train_df = pd.merge(train_df ,dummies_plans,left_index=True,right_index=True)
train_df = train_df.drop(columns='OtherInstallPlans')
train_df.head()

dummies_plans = pd.get_dummies(test_df['OtherInstallPlans'],prefix='plans')
dummies_plans.head()


test_df = pd.merge(test_df ,dummies_plans,left_index=True,right_index=True)
test_df = test_df.drop(columns='OtherInstallPlans')
test_df.head()

train_df

test_df

"""# 7) Build a k-NN classifier on training data and perform models selection using 5 fold cross validation

*  Initialize k-NN classifiers with **k= 5, 10, 15**
*  Calculate the cross validation scores using cross_al_score method, number of folds is 5. 
*  Note: Xval is performed on training data! Do not use test data in any way and do not separate a hold-out validation set, rather use cross-validation.

Documentation of the cross_val_score method:

[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)

*  Stores the average accuracies of these folds
*  Select the value of k using the cross validation results. 
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from statistics import mean

# k values
kVals = [5,10,15]

# Save the accuracies of each value of kVal in [accuracies] variable
accuracies = []

# Loop over values of k for the k-Nearest Neighbor classifier
for k in kVals:
  # Initialize a k-NN classifier with k neighbors
   neigh = KNeighborsClassifier(n_neighbors=k)

  # Calculate the 5 fold cross validation scores using cross_val_score
  # cv parameter: number of folds, in our case it must be 5
   scores = cross_val_score(neigh, train_df, train_label, cv=5)
   accuracies.append(mean(scores))

  # Stores the average accuracies of the scores in accuracies variable, you can use mean method


print(accuracies)

"""# 8) Retrain using all training data and test on test set

* Train a classifier with the chosen k value of the best classifier using **all training data**. 

Note:  k-NN training involves no explicit training, but this is what we would do after model selection with decision trees or any other ML approach (we had 5 diff. models -one for each fold - for each k in the previous step - dont know which one to submit. Even if we picked the best one, it does not use all training samples.

* Predict the labels of testing data 

* Report the accuracy 
"""



from sklearn.metrics import accuracy_score

# Train the best classifier using all training set
neigh = KNeighborsClassifier(n_neighbors=15)
neigh.fit(train_df, train_label)

test_predict= neigh.predict(test_df)
print('Test score for the best model:', accuracy_score(test_predict ,test_label))
# Print accuracy of test data

"""# 9) Bonus (5pts)

There is a limited bonus for any extra work that you may use and improve the above results. 

You may try a larger k values, scale input features, remove some features, .... Please **do not overdo**, maybe spend another 30-60min on this. The idea is not do an exhaustive search (which wont help your understanding of ML process), but just to give some extra points to those who may look at the problem a little more comprehensively. 

**If you obtain better results than the above, please indicate the best model you have found and the corresponding accuracy.**

E.g. using feature normalization ..... and removing .... features and using a value k=...., I have obtained ....% accuracy.

"""

from sklearn.preprocessing import StandardScaler
 
 # Used standart scaler to scale my data to 0 mean and unit variance
 scaler= StandardScaler()
 train_df_copy = pd.DataFrame(scaler.fit_transform(train_df),columns=train_df.columns)
 test_df_copy = pd.DataFrame(scaler.fit_transform(test_df),columns=test_df.columns)


from sklearn.metrics import accuracy_score


neigh = KNeighborsClassifier(n_neighbors=15)
neigh.fit(train_df_copy, train_label)

test_predict= neigh.predict(test_df_copy)
print('Test score for the best model:', accuracy_score(test_predict ,test_label))
# Print accuracy of test data

"""REPORT

In this homework, we are asked a create a model to classify each person
as risky or not to deserve credit from a bank. I used German Credit Risk 
datasets to train my model and test afterwards. As a classification method 
I used k-Nearest Neighbors algorthim with cross validation. 
Firsty, I seperated labels from train and test datas and mapped 2 to 1, and 1 to 0 simulatanouly which 0 is risky but 1 is not risky. Afterward, I converted categorical features to numerical features. I converted most of them according to order if it is possible. I looked at the data and labels, then decided to numerical values of features for ordering. For instance, some of the time, I di not give straightforward values such as 0 1 2 3. Instead, I gave weights for some cases. For instance, if the person has been working for 7+ years, I gave much more weight or has not been working yet, I gave negative weights. Also, I discarded personal statues as it does not affect much compared to other features. 
In training part, I used 3 different k values for KNN. I obtained accuracies such [0.6775, 0.70875, 0.71] respectively for k 5 10 15. As it is seen, I obtained slightly better accuracy. When I tested my model with k = 15, I obtained the accuracy 0.665. 

For bonus part, I scaled all the features to 0 mean and unit variance. When I test my data with, I obtained % 69.5 accuracy.
"""